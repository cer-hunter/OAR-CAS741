\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{gensymb}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newcounter{testnum} %Definition Number
\newcommand{\tthetestnum}{T\thetestnum}
\newcommand{\tref}[1]{T\ref{#1}}

\DeclareGraphicsExtensions{.png, .jpg, .bmp}

\begin{document}

\title{Optical Alphabet Recognition: System Verification and Validation Plan} 
\author{Hunter Ceranic}
\date{February 19, 2024}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
February 18, 2024 & 1.0 & Initial Revision\\
February 19, 2024 & 1.1 & Added Input Image Orientation Test\\
April 10, 2024 & 1.2 & Updated Document according to feedback from Primary and Secondary Reviewers\\
April 13, 2024 & 1.3 & Updated Document according to detailed comments from Dr. Smith\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  A & Assumption\\
  R & Requirement\\
  SRS & System Requirements Specification\\
  T & Test\\
  VnV & Verification and Validation\\
  OAR & Optical Alphabet Recognition\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

This document describes the Verification and Validation plan for the OAR Project. Testing of the software and its components
will be conducted in accordance with this document to improve confidence in the accuracy, understandibility and ability for the software 
to meet the requirements laid out in the Software Requirement Specifications (SRS).

\section{General Information}

\subsection{Summary}

The OAR project will result in software that is able to accurately classify input images of capital-letter English alphabet charcters using
logistic regression. An emphasis is placed on understandable code so that OAR can be a launch point for learning about optical
character recognition.

\subsection{Objectives} \label{Objectives}

The objective is to build confidence in the correctness of the software, and making sure it is comparable
to similar soultions (\textit{accuracy}). The correctness of the output is very easy for a user to evaluate themselves, so
it is essential the OAR project not only outputs the correct label, but is able to do so at a high confidence probability level.  
As a secondary goal, the \textit{understandability} of the code is important, as the project is designed as a learning tool.
This goal is a subset of making the code \textit{maintainable}, and will require both thorough code writing during implementation
and user input to validate. Parts of the code will rely on external libraries for operations like pre-processing input images,
however while it would be beneficial to verify the correctness of these libraries, this is out of the scope of the project and it will 
be assumed these external libraries will have been verified by their implementation teams.

\subsection{Relevant Documentation}

There are multiple design documents that provide in-depth details for understanding
the program being tested. These are as follows:

\begin{itemize}
  \item Problem Statement \citep{Prob_Statement}: States the problem we are trying to solve
    and introduces the topics that will need be verified.
  \item SRS \citep{SRS}: States the requirements, assumptions, models, and important 
  terminology that are used in the VnV plan.
  \item VnV Report \citep{VnV_report}: States what has been achieved from the VnV plan,
    and provides results or evidence to help build confidence for correctness.
  \item MG \citep{MG}: States the responsibilities and secrets for each module that will
    be evaluated by the VnV plan.
  \item MIS \citep{MIS}: Specifies the functional design and states what is needed for each
    module that will be evaluated by the VnV plan.
\end{itemize}

\section{Plan}

In this section, multiple plans are described to test and inspect the software with an emphasis 
on \textit{accuracy} and \textit{understandability}. Multiple approaches and perspectives will be employed by the VnV team (\ref{Team})
to help build confidence in the requirements, to avoid any missed important details, 
and to deliver on the outlined objectives (\ref{Objectives}).

\subsection{Verification and Validation Team} \label{Team}

The members of the VnV team as well their individual roles are listed in the following table:

\begin{table}[h!]
  \centering
  \begin{tabular}{|r|l|}
    \hline
    \textbf{Role} & \textbf{Name} \\ \hline
    Project Supervisor & Dr.\ Spencer Smith  \\ \hline
    Author             & Hunter Ceranic      \\ \hline
    Domain Expert      & Adrian Sochaniwsky  \\ \hline
    SRS Reviewer       & Phil Du             \\ \hline
    VnV Plan Reviewer  & Goafeng Zhou        \\ \hline
    MG + MIS Reviewer  & Yiding Li           \\ \hline
  \end{tabular}
  \caption{Table of the VnV Team Members}
  \label{table_vnv_team}
\end{table}

\subsection{SRS Verification Plan}

The SRS will be reviewed by the project supervisor, the SRS reviewer and the author. Most of the feedback has been provided 
through issues on GitHub, as annotated documents, or by verbal exchange. 
Throughout the development of the project, the author is expected 
to make changes needed to the resolve the issues. The reviewers may refer to the SRS checklist \citep{SRS_checklist}.
The key objective is to verify that the software requirements and the documentation are sound 
and coherent to the intended audience as defined in the SRS.

\subsection{Design Verification Plan}

Design decisions were made with a focus on performance and code understandability, so that the program could also be used 
as an educational tool for learning about image recognition. The design and implementation is documented in the 
MG\citep{MG}/MIS\citep{MIS} which will be verified with the MG 
and MIS \citep{MG_checklist, MIS_checklist} checklists.
The VnV team will provide their input using these checklists through GitHub issues. 

\subsection{Verification and Validation Plan Verification Plan}

The goal is to uncover any mistakes and reveal any risks (such as misconceptions or coverage gaps) 
through the supervision and 
review of the VnV team members. Once most of the work has been done, the work and
accompanying documentation shall undergo a vetting process: the VnV team
will check whether the documented testing plans and verification process have been 
accomplished and the requirements fulfilled. The author will then review the documents
against the VnV checklist \citep{VnV_checklist}, before a final review by the rest of the VnV team.

\subsection{Implementation Verification Plan}

Both automated and manual testing will be performed for this project. This will include checking for any unexpected outputs or bugs that may be encountered.
To ensure code consistency, linting tools will be implemented as described in \ref{Tools}. Furthermore,
tests for the program based on requirements will be performed using an automated test suite. Manual tests will be 
performed to benchmark the performance of the OAR project against already existing solutions.


\subsection{Automated Testing and Verification Tools} \label{Tools}

Continous integration will be implemented for this project using CircleCI, which 
is easily integrated with GitHub and Python the programming language for the project. CircleCI automates the testing 
pipeline for the project by building the project, running tests created using the pytest library, and linting,
on selected pushes or pulls to the repository \citep{CircleCI}. The test created with pytest are performed by predetermining potential user
inputs and comparing them with expected values. In addition to the continous integration suite, Flake 8 will be used as a 
linter for the code created through the VSCode IDE \citep{Flake8}.

\subsection{Software Validation Plan}

Software validation is beyond the scope of the OAR project due to the sheer amount of experimental data needed to be collected to 
validate the system behaviour not being feasible, given the time constraints. 

\section{System Tests}

This section outlines the tests that will be performed on the program to address the functional and nonfunctional requirements
found in the SRS \citep{SRS}. The tests and test images are located here:
\url{https://github.com/cer-hunter/OAR-CAS741/tree/main/tests}

	
\subsection{Tests for Functional Requirements}

In this section, the system tests that will be conducted are described in detail. These tests
will be used to verify the fulfillment of the requirements as listed in the SRS \citep{SRS}.
All of the tests listed here will be automatically performed unless otherwise stated.

\subsubsection{Input Image Type}

To satisfy R1 from the SRS \citep{SRS}, any input image of the following formats listed below shall be 
accepted provided they follow the input data constraints.

\begin{itemize}
  \item{PNG}
  \item{JPG}
  \item{BMP\\}
\end{itemize}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputImage}}: Test input for PNG/JPG/BMP format\\}
            
  Control: Automatic
            
  Initial State: OAR loaded and idle.
            
  Input: A subset of non-corrupt (valid) PNG, JPG, and BMP image files depicting valid label characters (see Table \ref{table_ext}, Figure \ref{Fig_A}).
            
  Output: The program should give a valid output as assigned by the oracle (the author).
            
  Test Case Derivation: These are some of the most common image file formats and should be compatible with the software.
            
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.
\end{enumerate}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputError}}: Test input for Invalid formats\\}
            
  Control: Automatic
            
  Initial State: OAR loaded and idle.
            
  Input: A subset of directory paths pointing to a PDF image file (see Table \ref{table_exterr}), an empty file path, and no file path.
            
  Output: The program should output a ValueError for all of these inputs.
            
  Test Case Derivation: These are some common input paths that would not be valid given the requirements.
            
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.
\end{enumerate}

\subsubsection{Unprocessed Input Images}
To satisfy R2 from the SRS \citep{SRS}, regardless of colorscale or pixel size any input image should be able to be accepted to 
be processed into a format usable by the program. In addition, pixel sizes are constrained to the following ranges:

\begin{itemize}
  \item{\textit{max}: 4096px $\times$ 4096px}
  \item{\textit{min}: 20px $\times$ 20px}
\end{itemize}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputColour}}: Test input for images with different colour schemes\\}
            
  Control: Automatic
            
  Initial State: OAR loaded and idle.
            
  Input: A set of non-corrupt (valid) image files depicting valid label characters (see Table \ref{table_colour}) in black and white (Figure \ref{Fig_A}), 
  grayscale (Figure \ref{Fig_grayA}), red (Figure \ref{Fig_redA}), green (Figure \ref{Fig_greenA}) and blue (Figure \ref{Fig_blueA}).

  
  Output:  The program should give a valid output as assigned by the oracle (the author), meaning some classification result regardless
  of whether it is correct.
            
  Test Case Derivation: These images cover the cases of grayscale, black and white and the primary colours of RGB images which should be compatible with the software.
            
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputSize}}: Test input for images with different pixel sizes\\}
            
  Control: Automatic
            
  Initial State: OAR loaded and idle.
            
  Input: A set of non-corrupt (valid) image files depicting valid label characters (see Table \ref{table_size}, Figure \ref{Fig_A}) of the following sizes:
  \begin{itemize}
    \item{8192px $\times$ 8192px}
    \item{4096px $\times$ 4096px}
    \item{20px $\times$ 20px}
    \item{10px $\times$ 10px}
  \end{itemize}
            
  Output: In the 8192px $\times$ 8192px and 10px $\times$ 10px cases the program should provide the user with a Value Error. In all other cases,
  the program should give a valid output as assigned by the oracle (the author), meaning some classification result regardless
  of whether it is correct.
            
  Test Case Derivation: These images cover the edge cases of acceptable pixel sizes, as well as outside of the range which should be compatible with the software.
            
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

\end{enumerate}

\subsubsection{Output Labels}

To satisfy R4 and R5 (and by extension R3) from the SRS \citep{SRS}, any input image should be properly classified with a confidence level from the set of 26 letters: {A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_outputLabel}}: Test Output of Each Possible Label\\}
            
  Control: Automatic
            
  Initial State: OAR loaded and idle.
            
  Input: A set of non-corrupt (valid) image files depicting all 26 valid label characters (see Table \ref{table_labels}, Figure \ref{Fig_A}).
            
  Output: The predicted label output should match the actual label as assigned by the oracle (the author).

  Test Case Derivation: These images cover the range of possible label outputs for the program.
            
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_outputDegen}}: Test Output of Degenerate Cases\\}
            
  Control: Automatic
            
  Initial State: OAR loaded and idle.
            
  Input: A set of non-corrupt (valid) image files (see Table \ref{table_degen}) depicting degenerate cases such as multiple letters (Figure \ref{abc_image}), a picture, and a blank image depicting no character (Figure \ref{Fig_blank}).
            
  Output: The output for all degenerate cases should state Not Classified.
            
  Test Case Derivation: These images cover the range of possible "valid" inputs for the program, that should not produce an associated labal.
            
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_inputAngle}}: Test Output for input images with different orientations\\}
              
    Control: Automatic
              
    Initial State: OAR loaded and idle.
              
    Input: A set of non-corrupt (valid) image files depicting valid classifiable label characters generated by paint.net (see Table \ref{table_angle}, Figure \ref{Fig_angleA}) at the following orientations:
    \begin{itemize}
      \item{$0\degree$}
      \item{$\pm 1\degree$}
      \item{$\pm 2\degree$}
      \item{$\pm 5\degree$}
      \item{$\pm 15\degree$}
      \item{$\pm 45\degree$}
      \item{$\pm 90\degree$}
      \item{$\pm 135\degree$}
      \item{$180\degree$}
    \end{itemize}
              
    Output: In the 0\% case, the predicted label should match the actual label as assigned by the oracle (the author). In all other cases,
    the resulting value is unknown, although likely it will be incorrect, as this test is a degenerate edge case test that is covered by A4.
              
    Test Case Derivation: These images are not compatible with the software according to A4 in the SRS \citep{SRS}, however detecting that orientation is
    skewed will be difficult in the scope of the project. Therefore, this test is performed to evaluate the performance of the program in 
    these degenerate cases.
              
    How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.
\end{enumerate}






\subsection{Tests for Nonfunctional Requirements}

The following tests will check if the nonfunctional requirements, as defined in the SRS \citep{SRS}, are 
met. The emphasis is on the \textit{accuracy} (NFR1) and \textit{understandability}, and 
therby \textit{maintainability} (NFR3), of the software. In the case of OAR, 
correctness and ease of understanding the code both help contribute to being a better learning tool.
To satisfy the \textit{usability} (NFR1) requirement, a quick user survey shall be
conducted to establish perspective on ease of use as expanded on below. As for \textit{portability} (NFR4),
the user should be run the software on their platform and environment of choice.


\subsubsection{Accuracy Testing}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_accuracy}}: Accuracy Performance Test\\}

  Type: Manual
            
  Condition: The OAR program, and a program built
  using the Logistic Regression Function from the scikit-learn library \citep{SKLearn}.
            
  Result: The result of the accuracy calculation should be output to be compared (see \ref{accuracy_calc})
            
  How test will be performed: The confusion matrices of both programs will be compared using the accuracy metric.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_misclass}}: Misclassification Performance Test\\}

  Type: Manual
            
  Condition: The OAR program, and a program built
  using the Logistic Regression Function from the scikit-learn library \citep{SKLearn}.
            
  Result: The result of the misclassification calculation should be output to be compared (see \ref{misclass_calc})
            
  How test will be performed: The confusion matrices of both programs will be compared using the misclassification metric.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_precision}}: Precision Performance Test\\}

  Type: Manual
            
  Condition: The OAR program, and a program built
  using the Logistic Regression Function from the scikit-learn library \citep{SKLearn}.
            
  Result: The result of the precision calculations should be output to be compared (see \ref{precision_calc})
            
  How test will be performed: The confusion matrices of both programs will be compared using the precision metric.
\end{enumerate} 
					

\subsubsection{Maintainability Testing}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_understandSurvey}}: Code Understandability Survey\\}
            
  Type: Manual
            
  Condition: A group of intended users, 
  documentation about the supporting libraries, and the survey questions on topics related 
  to the code quality and clarity.
            
  Result: The completed user surveys (see \ref{survey_understand}).
            
  Note: The information collected from the surveys
  will be aggregated and analyzed for any patterns to
  change or adapt the code to the results.
            
  How test will be performed: The users will be given a series of questions to evaluate 
  the style and ease of understanding of the code as well as perceptions about how easy it will 
  be to build upon the code for those who are learning.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_linters}}: Static Code Analysis\\}

  Type: Automatic
            
  Condition: The software code will be tested through the use of Flake 8 \citep{Flake8}.
            
  Result: The code linters should list zero warnings, and zero errors.
            
  How test will be performed: Flake 8 is a VSCode Extension and it's messages are present at all times while 
  viewing and writing code. A config file may be used to ignore E402 so that module level imports can be at the top of the file.

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_duckTest}}: Duck Test Code Review\\}

  Type: Manual
            
  Condition: The software source code and the code review checklist (see \ref{checklist_duckTest}).
            
  Result: The code should respect the defined code review checklist.
            
  How test will be performed: The author with walkthrough the code manually and check for qualities
    such as consistent styling, loose variables, variable scoping, unreachable code, 
    sufficient commenting, etc.
\end{enumerate}


\subsubsection{Portability Testing}

\begin{enumerate}

  \item{\textbf{T\refstepcounter{testnum}\thetestnum \label{T_dockerImage}}: Docker Image Test\\}
            
  Type: Automatic
            
  Condition: Code pushed to GitHub, connected to a CicleCI jobflow.
            
  Result: Program Built Successfully.
            
  Note: CircleCI tests are built and executed in a Docker Image, a container that enables the program to be run on Linux or Windows systems,
  and ensures libraries and dependencies needed to build the code can be installed.
            
  How test will be performed: CircleCI will automatically run this test whenever a jobflow is executed.
\end{enumerate} 

\subsection{Traceability Between Test Cases and Requirements}

Traceability matrices are used to simplify the process of identifying what needs to be changed 
if a component is modified. An ``X'' is used to indicate links between items in the table. 
When a component is changed, the elements marked with an ``X'' might need to be updated as well.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & R1
    & R2
    & R3
    & R4
    & R5
    & NFR1
    & NFR2
    & NFR3
    & NFR4
  \\ \hline
  \tref{T_inputImage}           &X&X& &X& & &X& & \\ \hline
  \tref{T_inputError}           &X&X& &X& & & & & \\ \hline
  \tref{T_inputColour}          &X&X& &X& & &X& & \\ \hline
  \tref{T_inputSize}            &X&X& &X& & & & & \\ \hline
  \tref{T_outputLabel}          &X&X&X&X&X& & & & \\ \hline
  \tref{T_outputDegen}          &X&X&X&X&X& & & & \\ \hline
  \tref{T_inputAngle}           &X&X&X&X&X& &X& & \\ \hline
  \tref{T_accuracy}             & & &X&X&X&X& & & \\ \hline
  \tref{T_misclass}             & & &X&X&X&X& & & \\ \hline
  \tref{T_precision}            & & &X&X&X&X& & & \\ \hline
  \tref{T_understandSurvey}     & & & & & & &X&X& \\ \hline
  \tref{T_linters}              & & & & & & & &X& \\ \hline
  \tref{T_duckTest}             & & & & & & & &X& \\ \hline
  \tref{T_dockerImage}          & & & & & & & & &X\\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Tests and Requirements}
  \label{Table:A_trace}
\end{table}

\newpage


\section{Unit Test Description}

This section focuses on automated testing for individual modules as defined by the
MG \cite{MG} and MIS \cite{MIS}.
The unit tests and test image files are located here:
\url{https://github.com/cer-hunter/OAR-CAS741/tree/main/tests}

\subsection{Unit Testing Scope}

All of the tests will be ran automatically through CircleCI \ref{CircleCI} unless otherwise stated.
The functionality of the Output Calculator (M4), Input Data Read (M5), and Input Classifier (M6) Modules are tested by the
system test cases listed above and as such, unit tests for these modules are not needed. Additionally since the Metrics (M11), Input Processing (M12),
and Graphical User Interface (M13) modules are all implemented by 3rd party libraries, in interest of time they will not be unit tested and it 
will be assumed that the functions they supply have already been verified. The OAR Model Testing Module (M10) will be tested manually, due to the nature of the
module creating the entire model taking an excessively long time to run (too long for free automated testing resources). Although it is possible to test GUIs and visual fidelity to some degree, this
is not planned here to due to limited resources (mainly time), and as such the Application Control (M2) and Graphics Display (M3) modules will not be tested.
Finally since the OAR Model Data (M7) Module is a record to be written and read from there are no direct functionality tests that can be performed 
(these are performed by tests on other modules). 
Unit Testing will focus mainly on the OAR Model Equations (M8) and OAR Model Training (M9) modules and a suite of inputs are provided for each function (see Table \ref{table_unittest}).

\subsection{Tests for Functional Requirements}

The tests for each module to be tested will be listed here, and each test will be part of an automated unit test suite.

\subsubsection{Module 8 - OAR Model Equations}
This module contributes to R3, R4, and R5 by providing the underlying theory and equations needed to construct the model and classify input images.
The test names are related to the function that it tests.

\begin{enumerate}

\item{\textbf{UT1}: Sigmoid}

Type: Automatic
					
Input: \textbf{String} (see Table \ref{table_unittest})
					
Output: \textit{ValueError}

Test Case Derivation: The input to the Sigmoid Function must be a float or integer.

How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.


\item{\textbf{UT2}: LogLossFunc}

Type: Automatic
					
Input: \textbf{Valid Integer, Invalid Float}; \textbf{Invalid Integer, Valid Float}; \textbf{String, String}
					
Output: \textit{ValueError}

Test Case Derivation: The input trueVal must be an integer either 0 or 1, and the input predVal must be a float $0 \leq \text{predVal} \leq 1$.

How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

\item{\textbf{UT3}: Predict}

Type: Automatic
					
Input: \textbf{Matrix 1, Matrix 2, ValidInteger}; \textbf{String, String, ValidInteger}; \textbf{Matrix 1, Matrix 1, String}
					
Output: \textit{ValueError}

Test Case Derivation: The inputs inputImage and weight must be matrices of the same size, and the input bias must be an integer or float.

How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

\item{\textbf{UT4}: GradientW}

Type: Automatic
					
Input: \textbf{Matrix 1, String, Matrix 1, ValidInteger, ValidInteger, ValidInteger}; \textbf{Matrix 1, InvalidInteger, Matrix 1, ValidInteger, ValidInteger, ValidInteger}; \textbf{Matrix 1, ValidInteger, Matrix 1, ValidInteger, String, ValidInteger}; \textbf{Matrix 1, ValidInteger, Matrix 1, ValidInteger, ValidInteger, String}
					
Output: \textit{ValueError}

Test Case Derivation: The inputs inputImage, weight and bias will be tested by \textbf{UT3} as the Predict Function is called in this function. 
The input trueVal must be an integer either 0 or 1, the input regParam must be a float, and the input trainSize must be an integer.

How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

\item{\textbf{UT5}: GradientB}

Type: Automatic
					
Input: \textbf{Matrix 1, String, Matrix 1, ValidInteger}; \textbf{Matrix 1, InvalidInteger, Matrix 1, ValidInteger}
					
Output: \textit{ValueError}

Test Case Derivation: The inputs inputImage, weight and bias will be tested by \textbf{UT3} as the Predict Function is called in this function. 
The input trueVal must be an integer either 0 or 1.

How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.
\end{enumerate}

\subsubsection{Module 9 - OAR Model Training}
This module contributes to R3 by providing the underlying theory and equations needed to train the classification model.
The test names are related to the function that it tests, and the inputs are derived directly from the code, rather than the MIS document.

\begin{enumerate}

  \item{\textbf{UT6}: Train}

  Type: Automatic
            
  Input: \textbf{Matrix 1, ValidInteger, Matrix 1, ValidInteger, ValidFloat, String, ValidInteger}
            
  Output: \textit{ValueError}
  
  Test Case Derivation: The inputs inputImage, trueVal, weight, bias, regParam, and trainSize will be tested by \textbf{UT3}, \textbf{UT4} and \textbf{UT5} as the Predict, GradientW and GradientB Functions are called in this function. 
  The input alpha must be a float.
  
  How test will be performed: The automatic test will be performed using pytest, executed by CircleCI.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

There are no Unit Tests for Non-Functionl Requirements.

\subsection{Traceability Between Test Cases and Modules}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & M1
    & M2
    & M3
    & M4
    & M5
    & M6
    & M7
    & M8
    & M9
    & M10
    & M11
    & M12
    & M13
  \\ \hline
  \tref{T_inputImage}           & & & & &X& & & & & & &X& \\ \hline
  \tref{T_inputError}           & & & & &X& & & & & & &X& \\ \hline
  \tref{T_inputColour}          & & & & &X& & & & & & &X& \\ \hline
  \tref{T_inputSize}            & & & & &X& & & & & & &X& \\ \hline
  \tref{T_outputLabel}          & & & &X&X&X&X&X& & & &X& \\ \hline
  \tref{T_outputDegen}          & & & &X&X&X&X&X& & & &X& \\ \hline
  \tref{T_inputAngle}           & & & &X&X&X&X&X& & & &X& \\ \hline
  \tref{T_accuracy}             & & & & & & &X& & &X&X& & \\ \hline
  \tref{T_misclass}             & & & & & & &X& & &X&X& & \\ \hline
  \tref{T_precision}            & & & & & & &X& & &X&X& & \\ \hline
  \tref{T_understandSurvey}     &X&X&X&X&X&X&X&X&X&X&X&X&X\\ \hline
  \tref{T_linters}              &X&X&X&X&X&X&X&X&X&X&X&X&X\\ \hline
  \tref{T_duckTest}             &X&X&X&X&X&X&X&X&X&X&X&X&X\\ \hline
  \tref{T_dockerImage}          &X&X&X&X&X&X&X&X&X&X&X&X&X\\ \hline
  \textbf{UT1-5}                & & & & & & & &X& & & & & \\ \hline
  \textbf{UT6}                  & & & & & & & & &X& & & & \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between the Tests and Requirements}
  \label{Table:A_trace}
\end{table}


\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

\subsection{Calculation Definitions}

\subsubsection{Accuracy Calculation} \label{accuracy_calc}

Using generated confusion matrices the metrics for a given label prediction can be produced. The percent of predictions made
that represent True Positive ($\mathit{TA}$), False Negative ($\mathit{FN}$), False Positive ($\mathit{FP}$), and True Negative ($\mathit{TN}$) values can be obtained 
from that we define the definition of accuracy as:
\begin{gather}
\frac{\mathit{TP} + \mathit{TN}}{\mathit{TP} + \mathit{TN} + \mathit{FP} + \mathit{FN}}
\end{gather}

We can then use this definition to evaluate the overall accuracy of the OAR model, by comparing confusion matrices of the OAR model 
and any Library model using the following equation which calculates the relative error between the two models:
\begin{gather}
\frac{|\mathit{OAR Accuracy} - \mathit{Library Accuracy}|}{|\mathit{Library Accuracy}|} \times 100\% < \epsilon
\end{gather}
\subsubsection{Misclassification Calculation} \label{misclass_calc}

Using generated confusion matrices the metrics for a given label prediction can be produced. The percent of predictions made
that represent True Positive ($\mathit{TA}$), False Negative ($\mathit{FN}$), False Positive ($\mathit{FP}$), and True Negative ($\mathit{TN}$) values can be obtained 
from that we define the definition of misclassification as:
\begin{gather}
\frac{\mathit{FP} + \mathit{FN}}{\mathit{TP} + \mathit{TN} + \mathit{FP} + \mathit{FN}}
\end{gather}
We can then use this definition to evaluate the overall misclassifications of the OAR model, by comparing confusion matrices of the OAR model 
and any Library model using the following equation which calculates the relative error between the two models:
\begin{gather}
\frac{|\mathit{OAR Misclassification} - \mathit{Library Misclassification}|}{|\mathit{Library Misclassification}|} \times 100\% < \epsilon
\end{gather}
\subsubsection{Precision Calculation} \label{precision_calc}

Using generated confusion matrices the metrics for a given label prediction can be produced. The percent of predictions made
that represent True Positive ($\mathit{TA}$), False Negative ($\mathit{FN}$), False Positive ($\mathit{FP}$), and True Negative ($\mathit{TN}$) values can be obtained 
from that we define the definition of precision as:
\begin{gather}
\frac{\mathit{TP}}{\mathit{TP} + \mathit{FP}}
\end{gather}
We can then use this definition to evaluate the overall precision of the OAR model, by comparing confusion matrices of the OAR model 
and any Library model using the following equation which calculates the relative error between the two models:
\begin{gather}
\frac{|\mathit{OAR Precision} - \mathit{Library Precision}|}{|\mathit{Library Precision}|} \times 100\% < \epsilon
\end{gather}
    
\subsection{Understandability Survey Questions} \label{survey_understand}

\begin{itemize}
  \item{On a scale from 1 to 10, how easy was it to understand the code?}
  \item{Were there any OAR specific functions or variable names that caused confusion?}
  \item{Were there any parts of the code that seemed not useful or uneccessary?}
  \item{What part of the code was the most confusing/unclear?}
  \item{Was the use of comments adequate?}
  \item{Do you think you understand the code enough to be able to build upon it further?}
  \item{Would you consider using the software as a learning or teaching tool?}
  \item{Do you have any extra comments or thoughts you'd like to share?}
\end{itemize}

\subsection{Duck Test Checklist} \label{checklist_duckTest}

\begin{itemize}
  \item{Does the code follow a consistent style?}
  \item{Are there any variables that should not be global?}
  \item{Are function names not too long (i.e. less than 40 characters)?}
  \item{Are most functions well named and clear in what they do?}
  \item{Are there any long lines of that can be split into multiple lines?}
  \item{Are global variables and functions documented?}
  \item{Is each function modular (not too long and sufficiently broken down to accomplish a single task)?}
  \item{Are comments plentiful, and written in a way that makes it easy for beginners to follow?}
  \item{Is the code sorted into separate files where reasonable?}
  \item{Is there any duplicate that code be avoided?}
  \item{Is there any use of jargon, domain-specific, or unclear terms?}
  \item{Is all of the code reachable?}
  \item{Is the control flow convoluted?}
  \item{Is there any unnecessarily obscure code? If necessary, is it commented and explained?}
  \item{Could programmer other than the author read the code, and sufficeintly understand it to build upon it?}
  \item{Is there any leftover commented code that is not useful?}
\end{itemize}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[A.bmp]{"A.bmp"} & .BMP file & Input Processed Successfully \\ \hline
    \hyperref[A.jpg]{"A.jpg"} & .JPG file & Input Processed Successfully \\ \hline
    \hyperref[A.png]{"A.png"} & .PNG file & Input Processed Successfully \\ \hline
  \end{tabular}
  \caption{Table of Valid Extension Images}
  \label{table_ext}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[A.pdf]{"A.pdf"} & .PDF file & ValueError \\ \hline
    \hyperref[Empty]{"Empty"} & Empty Folder & ValueError \\ \hline
    None & No Path Passed & ValueError \\ \hline
  \end{tabular}
  \caption{Table of Invalid Extension Images}
  \label{table_exterr}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[A.png]{"A.png"} & Black and White A & Input Processed Successfully \\ \hline
    \hyperref[A\_Blue.png]{"A\_Blue.png"} & Blue A & Input Processed Successfully \\ \hline
    \hyperref[A\_Green.png]{"A\_Green.png"} & Green A & Input Processed Successfully \\ \hline
    \hyperref[A\_Red.png]{"A\_Red.png"} & Red A & Input Processed Successfully \\ \hline
    \hyperref[A\_Gray.png]{"A\_Gray.png"} & Gray A & Input Processed Successfully \\ \hline
  \end{tabular}
  \caption{Table of Images with Different Colours}
  \label{table_colour}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[toobig.png]{"toobig.png"} & Image of A of size 8192px $\times$ 8192px & Value Error \\ \hline
    \hyperref[big.png]{"big.png"} & Image of A of size 4096px $\times$ 4096px & Input Processed Successfully \\ \hline
    \hyperref[small.png]{"small.png"} & Image of A of size 20px $\times$ 20px & Input Processed Successfully \\ \hline
    \hyperref[toosmall.png]{"toosmall.png"} & Image of A of size 10px $\times$ 10px & Value Error \\ \hline
  \end{tabular}
  \caption{Table of Images of Different Sizes in pixels}
  \label{table_size}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[A.jpg]{"A.jpg"} & Letter A & Classification and Confidence Level Displayed\\ \hline
    \hyperref[B.jpg]{"B.jpg"} & Letter B & Classification and Confidence Level Displayed\\ \hline
    \hyperref[C.jpg]{"C.jpg"} & Letter C & Classification and Confidence Level Displayed\\ \hline
    \hyperref[D.jpg]{"D.jpg"} & Letter D & Classification and Confidence Level Displayed\\ \hline
    \hyperref[E.jpg]{"E.jpg"} & Letter E & Classification and Confidence Level Displayed\\ \hline
    \hyperref[F.jpg]{"F.jpg"} & Letter F & Classification and Confidence Level Displayed\\ \hline
    \hyperref[G.jpg]{"G.jpg"} & Letter G & Classification and Confidence Level Displayed\\ \hline
    \hyperref[H.jpg]{"H.jpg"} & Letter H & Classification and Confidence Level Displayed\\ \hline
    \hyperref[I.jpg]{"I.jpg"} & Letter I & Classification and Confidence Level Displayed\\ \hline
    \hyperref[J.jpg]{"J.jpg"} & Letter J & Classification and Confidence Level Displayed\\ \hline
    \hyperref[K.jpg]{"K.jpg"} & Letter K & Classification and Confidence Level Displayed\\ \hline
    \hyperref[L.jpg]{"L.jpg"} & Letter L & Classification and Confidence Level Displayed\\ \hline
    \hyperref[M.jpg]{"M.jpg"} & Letter M & Classification and Confidence Level Displayed\\ \hline
    \hyperref[N.jpg]{"N.jpg"} & Letter N & Classification and Confidence Level Displayed\\ \hline
    \hyperref[O.jpg]{"O.jpg"} & Letter O & Classification and Confidence Level Displayed\\ \hline
    \hyperref[P.jpg]{"P.jpg"} & Letter P & Classification and Confidence Level Displayed\\ \hline
    \hyperref[Q.jpg]{"Q.jpg"} & Letter Q & Classification and Confidence Level Displayed\\ \hline
    \hyperref[R.jpg]{"R.jpg"} & Letter R & Classification and Confidence Level Displayed\\ \hline
    \hyperref[S.jpg]{"S.jpg"} & Letter S & Classification and Confidence Level Displayed\\ \hline
    \hyperref[T.jpg]{"T.jpg"} & Letter T & Classification and Confidence Level Displayed\\ \hline
    \hyperref[U.jpg]{"U.jpg"} & Letter U & Classification and Confidence Level Displayed\\ \hline
    \hyperref[V.jpg]{"V.jpg"} & Letter V & Classification and Confidence Level Displayed\\ \hline
    \hyperref[W.jpg]{"W.jpg"} & Letter W & Classification and Confidence Level Displayed\\ \hline
    \hyperref[X.jpg]{"X.jpg"} & Letter X & Classification and Confidence Level Displayed\\ \hline
    \hyperref[Y.jpg]{"Y.jpg"} & Letter Y & Classification and Confidence Level Displayed\\ \hline
    \hyperref[Z.jpg]{"Z.jpg"} & Letter Z & Classification and Confidence Level Displayed\\ \hline
  \end{tabular}
  \caption{Table of Images of Different Letters}
  \label{table_labels}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[ABC]{"ABC.jpg"} & Image of Letters A, B and C& Classification and Confidence level displayed stating "Not Classified"\\ \hline
    \hyperref[Picture]{"hunter_ok.jpg"} & Picture of the Author & Classification and Confidence level displayed stating "Not Classified" \\ \hline
    \hyperref[Blank]{"Blank.jpg"} & Blank Image & Classification and Confidence level displayed stating "Not Classified" \\ \hline
  \end{tabular}
  \caption{Table of Images of Degenerate Input Cases}
  \label{table_degen}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperlink} & \textbf{Description} & \textbf{Expected Output} \\ \hline
    \hyperref[0 degrees]{"angle0.png"} & Image at 0 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+1 degrees]{"angle1p.png"} & Image at +1 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-1 degrees]{"angle1n.png"} & Image at -1 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+2 degrees]{"angle2p.png"} & Image at +2 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-2 degrees]{"angle2n.png"} & Image at -2 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+5 degrees]{"angle5p.png"} & Image at +5 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-5 degrees]{"angle5n.png"} & Image at -5 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+15 degrees]{"angle15p.png"} & Image at +15 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-15 degrees]{"angle15n.png"} & Image at -15 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+45 degrees]{"angle45p.png"} & Image at +45 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-45 degrees]{"angle45n.png"} & Image at -45 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+90 degrees]{"angle90p.png"} & Image at +90 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-90 degrees]{"angle90n.png"} & Image at -90 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[+135 degrees]{"angle135p.png"} & Image at +135 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[-135 degrees]{"angle135n.png"} & Image at -135 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
    \hyperref[180 degrees]{"angle180.png"} & Image at 180 degrees Orientation & Classification and Confidence Level Displayed\\ \hline
  \end{tabular}
  \caption{Table of Images of Different Angles}
  \label{table_angle}
\end{table}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Name} & \textbf{Code Description} \\ \hline
    \textbf{String} & A string input that takes the form: \textit{"test"} \\ \hline
    \textbf{Valid Integer} & An integer of the value: 1 \\ \hline
    \textbf{Valid Float} & A float of the value: 0.5 \\ \hline
    \textbf{Invalid Integer} & An integer of the value: 2 \\ \hline
    \textbf{Invalid Float} & A float of the value: -5.4 \\ \hline
    \textbf{Matrix 1} & A matrix of ones of the size $3 \times 1$ \\ \hline
    \textbf{Matrix 2} & A matrix of zeros of the size $2 \times 2$ \\ \hline
    
  \end{tabular}
  \caption{Table of Unit Test Inputs}
  \label{table_unittest}
\end{table}

\subsection{Figures}

\begin{figure}[h!]
  \begin{center}
   \includegraphics[width=0.6\textwidth]{A}
  \caption{This image has been converted into .bmp, .jpg, and .png formats,
  and similar images exist for each letter of the English alphabet. This example picture can also be rescaled using paint.net}
  \label{Fig_A} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{A_Gray}
  \caption{This image depicts a Grayscale A to satisfy}
  \label{Fig_grayA} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{A_Red}
  \caption{This image depicts a Red A to satisfy}
  \label{Fig_redA} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{A_Green}
  \caption{This image depicts a Green A}
  \label{Fig_greenA} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{A_Blue}
  \caption{This image depicts a Blue A}
  \label{Fig_blueA} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{angle90p}
  \caption{This image has been rotated to a $90\degree$ angle. Rotations are performed using paint.net}
  \label{Fig_angleA} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{Blank}
  \caption{This is a blank image}
  \label{Fig_blank} 
  \end{center}
  \end{figure}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{ABC}
  \caption{This image depicts the characters A, B and C}
  \label{Fig_ABC} 
  \end{center}
  \end{figure}

\end{document}